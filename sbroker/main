#!/usr/bin/env python
import requests
import re
import os
from bs4 import BeautifulSoup

login = {
	'username':	None,
	'password':	None
}

url = {
	'login':	'https://www.sbroker.de/sbl/service/anmelden',
	'depot':	'https://www.sbroker.de/sbl/uebungsdepot/depot_standard',
	'depot_e':	'https://www.sbroker.de/sbl/uebungsdepot/depot_erweitert'
}

parser = {
	'value':	'Gesamtwert:',
	'buy_value':	'Gesamtkaufwert:',
	'delta_1d':	'Diff. abs. Vortag:'
}

depots = []

def parsedepot(soup, name, depot_id):
	depot = {
		'name':		name,
		'depot_id':	depot_id,
		'value':	soup.find('span', string=parser['value']).parent.parent.find(string=re.compile('EUR')),
		'buy_value':	soup.find('span', string=parser['buy_value']).parent.parent.find(string=re.compile('EUR')),
		'delta_1d':	soup.find('span', string=parser['delta_1d']).parent.parent.find(string=re.compile('EUR'))
	}
	depots.append(depot)

with open("{}/config".format(os.path.dirname(os.path.realpath(__file__))), 'r') as configfile:
	login.update({
		'username': configfile.readline().strip(),
		'password': configfile.readline().strip()
	})

with requests.Session() as s:
	s.headers.update({'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0'})
	page = s.get(url['login'])
	soup = BeautifulSoup(page.text, 'html.parser')
	form = soup.find('form', attrs={'name':'login'})
	postdata = {
		'_csrfToken':	form.find('input', attrs={'name':'_csrfToken'})['value'],
		'edited':	form.find('input', attrs={'name':'edited'})['value'],
		'ref':		form.find('input', attrs={'name':'ref'})['value']
	}
	postdata.update(login)
	
	s.headers.update({'Referer': 'https://www.sbroker.de/sbl/service/anmelden'})
	page = s.post(url['login'], data=postdata)
	
	page = s.get(url['depot_e'])
	soup = BeautifulSoup(page.text, 'html.parser')
	todo = []
	for elem in soup.find('select', attrs={'name':'pId'}).find_all('option'):
		try:
			elem['selected']
		except:
			todo.append({'name':elem.string, 'id':elem['value']})
		else:
			parsedepot(soup, elem.string, elem['value'])
	
	for elem in todo:
		page = s.get(url['depot_e'], params={'pId': elem['id']})
		soup = BeautifulSoup(page.text, 'html.parser')
		parsedepot(soup, elem['name'], elem['id'])
	
	for elem in depots:
		print('{}\tName: {}\n  Kaufwert: {}\n      Wert: {}\n   Diff 1d: {}\n\n'.format(elem['depot_id'], elem['name'], elem['buy_value'], elem['value'], elem['delta_1d']))
